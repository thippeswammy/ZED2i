{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639826f1-d45f-412c-9be4-a4d2d425a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy\n",
    "# !pip install torch\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1b8bd2-d2ba-455a-b8b8-041c2d736260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as tvtf\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights,MaskRCNN_ResNet50_FPN_V2_Weights\n",
    "# from torchvision.models.quantization import ResNet50_QuantizedWeights\n",
    "# from torchvision.utils import make_grid\n",
    "# from torchvision.io import read_image\n",
    "from pathlib import Path\n",
    "# from torchvision.utils import draw_bounding_boxes\n",
    "# from torchvision.utils import draw_segmentation_masks\n",
    "# from torchvision.utils import make_grid\n",
    "# from torchvision.io import read_image\n",
    "# from pathlib import Path\n",
    "\n",
    "# import stereo_image_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c537090b-27ac-47b9-8d25-eef7819b1213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are some helper functions\n",
    "# load image from file\n",
    "# preprocess image for input into mask rcnn model\n",
    "# display image\n",
    "# display image pair, to display two images\n",
    "\n",
    "'''\n",
    "  # img: This represents the image data loaded from the file using cv2.imread in the previous step.\n",
    "  # cv2.cvtColor(img, cv2.COLOR_BGR2RGB): This applies OpenCV's cv2.cvtColor function to convert the\n",
    "     color space of the image stored in img from BGR\n",
    "     (Blue, Green, Red - OpenCV's default format) to RGB (Red, Green, Blue) format.\n",
    "'''\n",
    "\n",
    "def load_img(filename):\n",
    "    img = cv2.imread(filename)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = tvtf.to_tensor(image)\n",
    "    image = image.unsqueeze(dim=0)\n",
    "    return image\n",
    "\n",
    "def display_image(image):\n",
    "    fig, axes = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    if image.ndim == 2:\n",
    "        axes.imshow(image, cmap='gray', vmin=0, vmax=255)\n",
    "    else:\n",
    "        axes.imshow(image)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_image_pair(first_image, second_image):\n",
    "    # this funciton from Computer vision course notes\n",
    "    # When using plt.subplots, we can specify how many plottable regions we want to create through nrows and ncols\n",
    "    # Here we are creating a subplot with 2 columns and 1 row (i.e. side-by-side axes)\n",
    "    # When we do this, axes becomes a list of length 2 (Containing both plottable axes)\n",
    "\n",
    "    '''\n",
    "      # fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 8)) creates a figure (fig) and a list of axes (axes) using Matplotlib's plt.subplots.\n",
    "      # nrows=1 specifies one row of subplots.\n",
    "      # ncols=2 specifies two columns of subplots, which means the images will be displayed side-by-side.\n",
    "      # figsize=(12, 8) sets the size of the figure in inches.\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "      # fig represents the entire visualization window.\n",
    "      # axes is a list containing individual plotting areas (subplots) within the figure. The number of elements in the list corresponds to the number of subplots created.\n",
    "      # Together, fig and axes provide the foundation for creating your visualization using Matplotlib.\n",
    "    '''\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n",
    "\n",
    "    # TODO: Call imshow on each of the axes with the first and second images\n",
    "    # Make sure you handle both RGB and grayscale images\n",
    "\n",
    "    '''\n",
    "      # Checking Image Dimension:\n",
    "\n",
    "      # if first_image.ndim == 2:: This condition checks the number of dimensions (ndim) of the image.\n",
    "      # If the image has two dimensions (ndim == 2), it indicates a grayscale image.\n",
    "    '''\n",
    "\n",
    "    # axes[0].imshow(first_image): This displays the first_image on the first subplot (axes[0]) using Matplotlib's imshow function.\n",
    "    # cmap='gray': This argument specifies the colormap to be used for displaying the grayscale image. A grayscale colormap is appropriate for images that only have intensity values.\n",
    "    # vmin=0 and vmax=255: These arguments set the minimum and maximum intensity values for the colormap. In this case, they ensure the grayscale values range from black (0) to white (255).\n",
    "\n",
    "    if first_image.ndim == 2:\n",
    "       axes[0].imshow(first_image, cmap='gray', vmin=0, vmax=255)\n",
    "    else:\n",
    "        axes[0].imshow(first_image)\n",
    "\n",
    "    if second_image.ndim == 2:\n",
    "        axes[1].imshow(second_image, cmap='gray', vmin=0, vmax=255)\n",
    "    else:\n",
    "        axes[1].imshow(second_image)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80616089-5e8a-4ad9-ae51-e6cf9b86b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these colours are used to draw boxes.\n",
    "\n",
    "COLOURS = [\n",
    "    tuple(int(colour_hex.strip('#')[i:i+2], 16) for i in (0, 2, 4))\n",
    "    for colour_hex in plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8193b54-b863-480c-98ad-9d78356ec0b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m left_eye \u001b[38;5;241m=\u001b[39m path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft_image_0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#'left_eye_demo2.jpg'\u001b[39;00m\n\u001b[0;32m      9\u001b[0m right_eye \u001b[38;5;241m=\u001b[39m path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright_image_0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#'right_eye_demo2.jpg'\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m left_img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_eye\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# left_img = cv2.resize(left_img, dsize=(sz1,sz2), interpolation=cv2.INTER_LINEAR)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m right_img \u001b[38;5;241m=\u001b[39m load_img(right_eye)\n",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_img\u001b[39m(filename):\n\u001b[0;32m     15\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(filename)\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# path = 'StereoCameraImages/2Images/'\n",
    "\n",
    "# left_eye = path + 'left_eye_' + d_calib + '.jpg' #'left_eye_demo2.jpg'\n",
    "# right_eye = path + 'right_eye_' + d_calib + '.jpg' #'right_eye_demo2.jpg'\n",
    "\n",
    "path = 'StereoCameraImages/2Images/'\n",
    "\n",
    "left_eye = path + 'left_image_0' + '.jpg' #'left_eye_demo2.jpg'\n",
    "right_eye = path + 'right_image_0' + '.jpg' #'right_eye_demo2.jpg'\n",
    "\n",
    "left_img = load_img(left_eye)\n",
    "# left_img = cv2.resize(left_img, dsize=(sz1,sz2), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "right_img = load_img(right_eye)\n",
    "# right_img = cv2.resize(right_img, dsize=(sz1,sz2), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "'''\n",
    "  # img.shape returns a tuple containing the image dimensions.\n",
    "  # [1] accesses the second element in the tuple, which corresponds to the width.\n",
    "  # [0] accesses the first element, which corresponds to the height\n",
    "'''\n",
    "\n",
    "sz1 ,sz2 = right_img.shape[1], right_img.shape[0]\n",
    "\n",
    "display_image_pair(left_img, right_img)\n",
    "\n",
    "imgs = [left_img, right_img]\n",
    "\n",
    "left_right = [preprocess_image(d).squeeze(dim=0) for d in imgs]\n",
    "\n",
    "print(right_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be10e831-ea77-4afa-9372-d4f2efcc1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the default weights and set up the model\n",
    "'''\n",
    "weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=weights)\n",
    "\n",
    "_ = model.eval()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48391efc-d165-4111-822e-c4a040a9f283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Assuming your model is already created and in evaluation mode (as in your previous code)\n",
    "# # model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
    "# model.eval()\n",
    "\n",
    "# # Specify the path to save the model (replace with your desired location)\n",
    "# save_path = \"/MaskrCNN_model.pt\"\n",
    "\n",
    "# # Save the model\n",
    "# torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de4add-51e3-4c11-b9f9-04fcc64218eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the default weights and set up the model\n",
    "import torch\n",
    "\n",
    "weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=weights)\n",
    "\n",
    "# Specify the path to save the model (replace with your desired location)\n",
    "save_path = \"MaskrCNN_model.pt\"\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), save_path)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab1a90-e8cf-451b-91b2-4b71dfd5997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"MaskrCNN_model.pt\"\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "# Set the model to evaluation mode\n",
    "_=model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e009313c-f65a-4601-a757-3d1601af1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions returns the detections\n",
    "# det is the boxes, top left and bottom right cooridinates\n",
    "# lbls are the class labels\n",
    "# scores are the confidence. We use 0.5 as default\n",
    "# masks are the segmentation masks.\n",
    "\n",
    "def get_detections(maskrcnn, imgs, score_threshold=0.5): #person, dog, elephan, zebra, giraffe, toilet\n",
    "    ''' Runs maskrcnn over all frames in vid, storing the detections '''\n",
    "    # Record how long the video is (in frames)\n",
    "    det = []\n",
    "    lbls = []\n",
    "    scores = []\n",
    "    masks = []\n",
    "\n",
    "    for img in imgs:\n",
    "        with torch.no_grad():\n",
    "            result = maskrcnn(preprocess_image(img))[0]\n",
    "            '''\n",
    "              result = maskrcnn(preprocess_image(img))[0]: This line passes the preprocessed image (preprocess_image(img))\n",
    "              through the Mask R-CNN model (maskrcnn). The [0] indexing extracts the first element of the returned result,\n",
    "              which likely contains the model's predictions.\n",
    "            '''\n",
    "\n",
    "        mask = result[\"scores\"] > score_threshold\n",
    "        # print(result)\n",
    "        print(mask)\n",
    "        '''\n",
    "          mask = result[\"scores\"] > score_threshold: This line creates a mask that filters the detections based on their\n",
    "          confidence scores (result[\"scores\"]). Only detections with scores higher than the specified\n",
    "          score_threshold (default 0.5) are kept.\n",
    "        '''\n",
    "\n",
    "        # boxes = result[\"boxes\"][mask].detach().cpu().numpy()\n",
    "        # det.append(boxes)\n",
    "\n",
    "        '''\n",
    "          # The line boxes = result[\"boxes\"][mask].detach().cpu().numpy() in the get_detections function extracts the\n",
    "          bounding boxes for the detections that pass the confidence score threshold. Here's a detailed breakdown:\n",
    "        '''\n",
    "        # list[\"columName\"][\"condiction\"]\n",
    "        det.append(result[\"boxes\"][mask].detach().cpu().numpy())\n",
    "        lbls.append(result[\"labels\"][mask].detach().cpu().numpy())\n",
    "        scores.append(result[\"scores\"][mask].detach().cpu().numpy())\n",
    "        masks.append(result[\"masks\"][mask]) #I want this as a tensor\n",
    "        # masks.append(result[\"masks\"][mask].detach().cpu().numpy())\n",
    "        # det is bounding boxes, lbls is class labels, scores are confidences and masks are segmentation masks\n",
    "\n",
    "        \n",
    "    print(lbls)\n",
    "    return det, lbls, scores, masks\n",
    "\n",
    "#det[0] are the bounding boxes in the left image\n",
    "#det[1] are the bounding boxes in the right image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb2131d-c9b0-4506-ad12-2fee22e2d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "det, lbls, scores, masks = get_detections(model,imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3ccf5-0496-4a50-9260-1bd6f0022572",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(weights.meta[\"categories\"])[lbls[0]])\n",
    "print(np.array(weights.meta[\"categories\"])[lbls[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b73a8-37ec-45e4-8bc1-f5f26028a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#draws the bounding boxes\n",
    "def draw_detections(img, det, colours=COLOURS, obj_order = None):\n",
    "  # i starts from 0, len(det), (tlx, tly, brx, bry) are postion\n",
    "    for i, (tlx, tly, brx, bry) in enumerate(det):\n",
    "        if obj_order is not None and len(obj_order) < i:\n",
    "            i = obj_order[i]\n",
    "        i %= len(colours)\n",
    "        c = colours[i]\n",
    "        # cv2.rectangle() use to draw the rectangle for img\n",
    "        cv2.rectangle(img, (tlx, tly), (brx, bry), color=colours[i], thickness=2)\n",
    "\n",
    "\n",
    "#annotate the class labels\n",
    "def annotate_class(img, det, lbls, conf=None, colours=COLOURS, class_map=weights.meta[\"categories\"]):\n",
    "    for i, ( tlx, tly, brx, bry) in enumerate(det):\n",
    "        txt = class_map[lbls[i]]\n",
    "        if conf is not None:\n",
    "            txt += f' {conf[i]:1.3f}'\n",
    "        # A box with a border thickness draws half of that thickness to the left of the\n",
    "        # boundaries, while filling fills only within the boundaries, so we expand the filled\n",
    "        # region to match the border\n",
    "        offset = 1\n",
    "\n",
    "        # again drawing the rectangle with filled color inside\n",
    "        cv2.rectangle(img,\n",
    "                      (tlx-offset, tly-offset+12),\n",
    "                      (tlx-offset+len(txt)*12, tly),\n",
    "                      color=colours[i%len(colours)],\n",
    "                      thickness=cv2.FILLED)\n",
    "\n",
    "        # writing the text that is lable in side filled rectangle in white color text\n",
    "        ff = cv2.FONT_HERSHEY_PLAIN\n",
    "        cv2.putText(img, txt, (tlx, tly-1+12), fontFace=ff, fontScale=1.0, color=(255,)*3)\n",
    "\n",
    "\n",
    "def draw_instance_segmentation_mask(img, masks):\n",
    "    ''' Draws segmentation masks over an img '''\n",
    "    seg_colours = np.zeros_like(img, dtype=np.uint8)\n",
    "    for i, mask in enumerate(masks):\n",
    "        col = (mask[0, :, :, None] * COLOURS[i])\n",
    "        seg_colours = np.maximum(seg_colours, col.astype(np.uint8))\n",
    "    cv2.addWeighted(img, 0.75, seg_colours, 0.75, 1.0, dst=img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc34186-7ffd-4055-ba85-0c34ab391bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "# imgs1 = imgs.copy()\n",
    "\n",
    "for i, imgi in enumerate(imgs):\n",
    "    img = imgi.copy()\n",
    "    deti = det[i].astype(np.int32)\n",
    "    draw_detections(img,deti)\n",
    "    masksi = masks[i].detach().cpu().numpy()\n",
    "    # print(masksi)\n",
    "    annotate_class(img,deti,lbls[i])\n",
    "    # draw_instance_segmentation_mask(img, masksi)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'Frame #{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc35dfe-67ad-41c8-9fec-cd67cbaca474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw with masks\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "# imgs2 = imgs.copy()\n",
    "\n",
    "for i, imgi in enumerate(imgs):\n",
    "    img = imgi.copy()\n",
    "\n",
    "    deti = det[i].astype(np.int32)\n",
    "    draw_detections(img,deti)\n",
    "    masks[i][masks[i]<0.7]=0\n",
    "    masksi = masks[i].detach().cpu().numpy()\n",
    "    annotate_class(img,deti,lbls[i])\n",
    "    draw_instance_segmentation_mask(img, masksi)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'Frame #{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed3fe3-91ba-4fac-a521-e19b602a5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get centr, top left and bottom right of boxes\n",
    "\n",
    "def tlbr_to_center1(boxes):\n",
    "    points = []\n",
    "    for tlx, tly, brx, bry in boxes:\n",
    "        cx = (tlx+brx)/2\n",
    "        cy = (tly+bry)/2\n",
    "        points.append([cx, cy])\n",
    "    return points\n",
    "\n",
    "def tlbr_to_corner(boxes):\n",
    "    points = []\n",
    "    for tlx, tly, brx, bry in boxes:\n",
    "        cx = (tlx+tlx)/2\n",
    "        cy = (tly+tly)/2\n",
    "        points.append((cx, cy))\n",
    "    return points\n",
    "\n",
    "def tlbr_to_corner_br(boxes):\n",
    "    points = []\n",
    "    for tlx, tly, brx, bry in boxes:\n",
    "        cx = (brx+brx)/2\n",
    "        cy = (bry+bry)/2\n",
    "        points.append((cx, cy))\n",
    "    return points\n",
    "\n",
    "def tlbr_to_area(boxes):\n",
    "    areas = []\n",
    "    for tlx, tly, brx, bry in boxes:\n",
    "        cx = (brx-tlx)\n",
    "        cy = (bry-tly)\n",
    "        areas.append(abs(cx*cy))\n",
    "    return areas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8dbba2-222a-4720-9fc2-445cec9f132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    # [[159.40157   27.520174 241.03233  226.7395  ]\n",
    "    # [  0.         0.       112.57273  157.12044 ]\n",
    "    # [240.61845  168.571    270.16025  219.13237 ]]\n",
    "\n",
    "\n",
    "    # it in the form of[[Tx1,Ty1,Bx1,By1],[Tx2,Ty2,Bx2,By2],[Tx3,Ty3,Bx3,By3]]\n",
    "    # where Tx : top x-axis position\n",
    "    # where Ty : top y-axis position\n",
    "    # where Bx : bottom x-axis position\n",
    "    # where By : bottom y-axis position\n",
    "    # where the 1,2,3...n are the n number item detected\n",
    "\n",
    "'''\n",
    "print(det[0],(\"\\n\")*3)\n",
    "print(tlbr_to_center1(det[0]),\"\\n\\n\\n\")\n",
    "print(np.array(tlbr_to_center1(det[0])),\"\\n\\n\\n\")\n",
    "print(np.array(tlbr_to_center1(det[0]))[:,0])\n",
    "# this gives the center point position of x-axis of each item detected\n",
    "# [200.21694946  56.28636551 255.38934326]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9eb9fe-a95e-45b2-b895-9146e5ca9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all distances from every object box to every other object box\n",
    "#left image is boxes[0]\n",
    "#right image is boxes[1]\n",
    "\n",
    "#do broad casting.\n",
    "#in python, col vector - row vector gives matrix:\n",
    "# [a] - [c,d] = [a-c, a-d]\n",
    "# [b]           [b-c, b-d]\n",
    "\n",
    "def get_horiz_dist_centre(boxes):\n",
    "    pnts1 = np.array(tlbr_to_center1(boxes[0]))[:,0]\n",
    "    pnts2 = np.array(tlbr_to_center1(boxes[1]))[:,0]\n",
    "    return pnts1[:,None] - pnts2[None]\n",
    "\n",
    "def get_horiz_dist_corner_tl(boxes):\n",
    "    pnts1 = np.array(tlbr_to_corner(boxes[0]))[:,0]\n",
    "    pnts2 = np.array(tlbr_to_corner(boxes[1]))[:,0]\n",
    "    return pnts1[:,None] - pnts2[None]\n",
    "\n",
    "def get_horiz_dist_corner_br(boxes):\n",
    "    pnts1 = np.array(tlbr_to_corner_br(boxes[0]))[:,0]\n",
    "    pnts2 = np.array(tlbr_to_corner_br(boxes[1]))[:,0]\n",
    "    return pnts1[:,None] - pnts2[None]\n",
    "\n",
    "def get_vertic_dist_centre(boxes):\n",
    "    pnts1 = np.array(tlbr_to_center1(boxes[0]))[:,1]\n",
    "    pnts2 = np.array(tlbr_to_center1(boxes[1]))[:,1]\n",
    "    return pnts1[:,None] - pnts2[None]\n",
    "\n",
    "def get_area_diffs(boxes):\n",
    "    pnts1 = np.array(tlbr_to_area(boxes[0]))\n",
    "    pnts2 = np.array(tlbr_to_area(boxes[1]))\n",
    "    return abs(pnts1[:,None] - pnts2[None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f5edf-5db6-466a-9f0c-e538b5888225",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get distance bentween corner and centre\n",
    "\n",
    "centre = sz1/2\n",
    "\n",
    "def get_dist_to_centre_tl(box, cntr = centre):\n",
    "    pnts = np.array(tlbr_to_corner(box))[:,0]\n",
    "    return abs(pnts - cntr)\n",
    "\n",
    "\n",
    "def get_dist_to_centre_br(box, cntr = centre):\n",
    "    pnts = np.array(tlbr_to_corner_br(box))[:,0]\n",
    "    return abs(pnts - cntr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a7ea2-c5fb-4c93-ae99-724a5b515962",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = get_dist_to_centre_br(det[0])\n",
    "tmp2 = get_dist_to_centre_br(det[1])\n",
    "print(tmp1)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac35790-62fd-4411-84bc-91b682d812e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tracking cost function.\n",
    "# consists of theree parts.\n",
    "#  1. The vertical move up and down of object centre of mass. Scale this up because we do not expect this to be very much.\n",
    "#  2. The move left or right by the object. We only expect it to move right (from the left eye image). So penalise if it moves left.\n",
    "#  3. The difference in area of pixels. Area of image is width x height, so divide by height, there for this will have max value of width\n",
    "\n",
    "def get_cost(boxes, lbls = None, sz1 = 400):\n",
    "    alpha = sz1; beta  = 10; gamma = 5\n",
    "\n",
    "    #vertical_dist, scale by gamma since can't move up or down\n",
    "    vert_dist = gamma*abs(get_vertic_dist_centre(boxes))\n",
    "\n",
    "    #horizonatl distance.\n",
    "    horiz_dist = get_horiz_dist_centre(boxes)\n",
    "\n",
    "    #increase cost if object has moved from right to left.\n",
    "    horiz_dist[horiz_dist<0] = beta*abs(horiz_dist[horiz_dist<0])\n",
    "\n",
    "    #area of box\n",
    "    area_diffs = get_area_diffs(boxes)/alpha\n",
    "\n",
    "    cost = np.array([vert_dist,horiz_dist,area_diffs])\n",
    "\n",
    "    cost=cost.sum(axis=0)\n",
    "\n",
    "    #add penalty term for different object classes\n",
    "    if lbls is not None:\n",
    "        for i in range(cost.shape[0]):\n",
    "            for j in range(cost.shape[1]):\n",
    "                if (lbls[0][i]!=lbls[1][j]):\n",
    "                    cost[i,j]+=150\n",
    "    return cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc920084-ac68-45be-8641-cd8ffa1de1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_with_com(masks, lbls = None,prob_thresh = 0.7):\n",
    "    alpha = 240; beta  = 10; gamma = 5\n",
    "\n",
    "    #left masks\n",
    "    mask_bool = masks[0] > prob_thresh\n",
    "    mask_bool = mask_bool.squeeze(1)\n",
    "    #right masks\n",
    "    mask_bool2 = masks[1] > prob_thresh\n",
    "    mask_bool2 = mask_bool2.squeeze(1)\n",
    "\n",
    "    #left params\n",
    "    #com1 is center of mass of height\n",
    "    #com2 is center of mass of width\n",
    "    mask_size = (mask_bool).sum(dim=[1,2])\n",
    "    mask_com_matrix_1 = torch.tensor(range(mask_bool.shape[1]))\n",
    "    com1 = ((mask_com_matrix_1.unsqueeze(1))*mask_bool).sum(dim=[1,2])/mask_size\n",
    "    mask_com_matrix_2 = torch.tensor(range(mask_bool.shape[2]))\n",
    "    com2 = ((mask_com_matrix_2.unsqueeze(0))*mask_bool).sum(dim=[1,2])/mask_size\n",
    "\n",
    "    left_params = torch.stack((com1, com2, mask_size)).transpose(1,0)\n",
    "\n",
    "    #get right params\n",
    "    mask_size2 = (mask_bool2).sum(dim=[1,2])\n",
    "    mask_com_matrix_12 = torch.tensor(range(mask_bool2.shape[1]))\n",
    "    com12 = ((mask_com_matrix_12.unsqueeze(1))*mask_bool2).sum(dim=[1,2])/mask_size2\n",
    "    mask_com_matrix_22 = torch.tensor(range(mask_bool2.shape[2]))\n",
    "    com22 = ((mask_com_matrix_22.unsqueeze(0))*mask_bool2).sum(dim=[1,2])/mask_size2\n",
    "\n",
    "    right_params = torch.stack((com12, com22, mask_size2)).transpose(1,0)\n",
    "\n",
    "    #calculate cost function\n",
    "    cost = (left_params[:,None] - right_params[None])\n",
    "    #scale counts\n",
    "    cost[:,:,2]=abs(cost[:,:,2])/alpha\n",
    "\n",
    "    #can't move right, can only move left\n",
    "    cost[cost[:,:,1]<0] = beta*abs(cost[cost[:,:,1]<0])\n",
    "\n",
    "    #move up and down, take abs vals\n",
    "    cost[:,:,0] = gamma*abs(cost[:,:,0])\n",
    "    # print(cost.shape)\n",
    "    cost = cost.sum(dim=2)\n",
    "    if lbls is not None:\n",
    "        for i in range(cost.shape[0]):\n",
    "            for j in range(cost.shape[1]):\n",
    "                if (lbls[0][i]!=lbls[1][j]):\n",
    "                    cost[i,j]+=100\n",
    "                    # print(lbls[0][i], lbls[1][j])\n",
    "    return cost\n",
    "\n",
    "\n",
    "def get_horiz_dist(masks, prob_thresh = 0.7):\n",
    "    # gets the horizontal distance between the centre of mass for each object\n",
    "    #left masks\n",
    "    mask_bool = masks[0] > prob_thresh\n",
    "    mask_bool = mask_bool.squeeze(1)\n",
    "    #right masks\n",
    "    mask_bool2 = masks[1] > prob_thresh\n",
    "    mask_bool2 = mask_bool2.squeeze(1)\n",
    "\n",
    "    #left params\n",
    "    #com1 is center of mass of height\n",
    "    #com2 is center of mass of width\n",
    "    mask_size = (mask_bool).sum(dim=[1,2])\n",
    "    mask_com_matrix_1 = torch.tensor(range(mask_bool.shape[1]))\n",
    "    com1 = ((mask_com_matrix_1.unsqueeze(1))*mask_bool).sum(dim=[1,2])/mask_size\n",
    "    mask_com_matrix_2 = torch.tensor(range(mask_bool.shape[2]))\n",
    "    com2 = ((mask_com_matrix_2.unsqueeze(0))*mask_bool).sum(dim=[1,2])/mask_size\n",
    "\n",
    "    left_params = torch.stack((com1, com2, mask_size)).transpose(1,0)\n",
    "\n",
    "    #get right params\n",
    "    mask_size2 = (mask_bool2).sum(dim=[1,2])\n",
    "    mask_com_matrix_12 = torch.tensor(range(mask_bool2.shape[1]))\n",
    "    com12 = ((mask_com_matrix_12.unsqueeze(1))*mask_bool2).sum(dim=[1,2])/mask_size2\n",
    "    mask_com_matrix_22 = torch.tensor(range(mask_bool2.shape[2]))\n",
    "    com22 = ((mask_com_matrix_22.unsqueeze(0))*mask_bool2).sum(dim=[1,2])/mask_size2\n",
    "\n",
    "    right_params = torch.stack((com12, com22, mask_size2)).transpose(1,0)\n",
    "\n",
    "    #calculate cost function\n",
    "    cost = (left_params[:,None] - right_params[None])\n",
    "    return cost[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fa1cc-228a-4acb-85d7-977e97ef9f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tracks(cost):\n",
    "    return scipy.optimize.linear_sum_assignment(cost)\n",
    "\n",
    "\n",
    "def get_tracks_ij(cost):\n",
    "    tracks = scipy.optimize.linear_sum_assignment(cost)\n",
    "    return [[i,j] for i, j in zip(*tracks)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b4302-2e52-4d67-9405-0255aba4b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = get_cost(det, lbls = lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f66c94-44bc-47af-aa13-c0f907e3a12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = scipy.optimize.linear_sum_assignment(cost)\n",
    "print(tracks)\n",
    "\n",
    "h_d = [[np.array(weights.meta[\"categories\"])[lbls[0]][i],np.array(weights.meta[\"categories\"])[lbls[1]][j]] for i, j in zip(*tracks)]\n",
    "print(np.array(weights.meta[\"categories\"])[lbls[0]])\n",
    "print(h_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e83ef79-a875-4ba2-aa75-bfafe02d1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we take the corner that is closest to the centre. This is because the other corner might be going off the image.\n",
    "\n",
    "dists_tl =  get_horiz_dist_corner_tl(det)\n",
    "dists_br =  get_horiz_dist_corner_br(det)\n",
    "\n",
    "final_dists = []\n",
    "dctl = get_dist_to_centre_tl(det[0])\n",
    "dcbr = get_dist_to_centre_br(det[0])\n",
    "\n",
    "for i, j in zip(*tracks):\n",
    "    if dctl[i] < dcbr[i]:\n",
    "        final_dists.append((dists_tl[i][j],np.array(weights.meta[\"categories\"])[lbls[0]][i]))\n",
    "\n",
    "    else:\n",
    "        final_dists.append((dists_br[i][j],np.array(weights.meta[\"categories\"])[lbls[0]][i]))\n",
    "\n",
    "\n",
    "final_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb68679-7ca1-444b-af99-faeb45f5380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get focal length\n",
    "# for 50cm away bottle image, we had 38.44 pixels between bottle boxes\n",
    "# and for 30cm away bottle image we had 68.75 pixels between left and right bottles\n",
    "fl = 30-38.44*50/68.75\n",
    "print(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51556f-a151-4eb2-abf7-22989014915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calibrate theta. cameras are 7.05 cms apart\n",
    "tantheta = (1/(50-fl))*(7.05/2)*sz1/38.44\n",
    "print(tantheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dda98b-5868-4418-93df-c4e9916d6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final distances as list\n",
    "fd = [i for (i,j) in final_dists]\n",
    "print(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26781b45-a388-4dba-b81d-5e1a6c316bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the distance away\n",
    "dists_away = (7.05/2)*sz1*(1/tantheta)/np.array(fd)+fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6b09e6-705e-4cc3-872c-9e1894158bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dist = []\n",
    "for i in range(len(dists_away)):\n",
    "    cat_dist.append(f'{np.array(weights.meta[\"categories\"])[lbls[0]][i]} {dists_away[i]:.1f}cm')\n",
    "    print(f'{np.array(weights.meta[\"categories\"])[lbls[0]][i]} is {dists_away[i]:.1f}cm away')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1748a10-70c2-434e-bcda-d1cbfa70eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotate the class labels\n",
    "def annotate_class2(img, det, lbls,class_map, conf=None,  colours=COLOURS):\n",
    "    for i, ( tlx, tly, brx, bry) in enumerate(det):\n",
    "        txt = class_map[i]\n",
    "        if conf is not None:\n",
    "            txt += f' {conf[i]:1.3f}'\n",
    "        # A box with a border thickness draws half of that thickness to the left of the\n",
    "        # boundaries, while filling fills only within the boundaries, so we expand the filled\n",
    "        # region to match the border\n",
    "        offset = 1\n",
    "\n",
    "        cv2.rectangle(img,\n",
    "                      (tlx-offset, tly-offset+12),\n",
    "                      (tlx-offset+len(txt)*12, tly),\n",
    "                      color=colours[i%len(colours)],\n",
    "                      thickness=cv2.FILLED)\n",
    "\n",
    "        ff = cv2.FONT_HERSHEY_PLAIN\n",
    "        cv2.putText(img, txt, (tlx, tly-1+12), fontFace=ff, fontScale=1.0, color=(255,)*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc89e8-f5d8-436a-bf5c-98089722934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.array(cat_dist)[(tracks[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec88dd-4d66-4bd4-8172-de24e6f853aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "t1 = [list(tracks[1]), list(tracks[0])]\n",
    "\n",
    "for i, imgi in enumerate(imgs):\n",
    "    img = imgi.copy()\n",
    "    deti = det[i].astype(np.int32)\n",
    "    draw_detections(img,deti[list(tracks[i])], obj_order=list(t1[i]))\n",
    "    annotate_class2(img,deti[list(tracks[i])],lbls[i][list(tracks[i])],cat_dist)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'Frame #{i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bc3a9-7e07-4396-8e7a-204d9e09af4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d0aa55-73c4-4a9c-95e1-3ad192e13fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
