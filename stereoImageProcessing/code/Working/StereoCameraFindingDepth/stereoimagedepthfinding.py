# -*- coding: utf-8 -*-
"""StereoImageDepthFinding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18l7_-jDsnHELWmkIU3nSWgJPR4qzQOn3

### Stereo vision
version 6

See for model
https://pytorch.org/vision/stable/models/mask_rcnn.html

https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html#torchvision.models.detection.maskrcnn_resnet50_fpn

See for visualisation https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html#sphx-glr-auto-examples-plot-visualization-utils-py
"""

import copy
import math


import numpy as np
import cv2
import matplotlib.pyplot as plt
import scipy
import scipy.optimize
import torch
import torchvision
import torchvision.transforms.functional as tvtf
from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights,MaskRCNN_ResNet50_FPN_V2_Weights
# from torchvision.models.quantization import ResNet50_QuantizedWeights
# from torchvision.utils import make_grid
# from torchvision.io import read_image
from pathlib import Path
# from torchvision.utils import draw_bounding_boxes
# from torchvision.utils import draw_segmentation_masks
# from torchvision.utils import make_grid
# from torchvision.io import read_image
# from pathlib import Path

# import stereo_image_utils

# here are some helper functions
# load image from file
# preprocess image for input into mask rcnn model
# display image
# display image pair, to display two images

'''
  # img: This represents the image data loaded from the file using cv2.imread in the previous step.
  # cv2.cvtColor(img, cv2.COLOR_BGR2RGB): This applies OpenCV's cv2.cvtColor function to convert the-
    -color space of the image stored in img from BGR(Blue, Green, Red - OpenCV's default format) to RGB (Red, Green, Blue) format.
'''

def load_img(filename):
    img = cv2.imread(filename)
    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

def preprocess_image(image):
    image = tvtf.to_tensor(image)
    image = image.unsqueeze(dim=0)
    return image

def display_image(image):
    fig, axes = plt.subplots(figsize=(12, 8))

    if image.ndim == 2:
        axes.imshow(image, cmap='gray', vmin=0, vmax=255)
    else:
        axes.imshow(image)

    plt.show()


def display_image_pair(first_image, second_image):
    # this funciton from Computer vision course notes
    # When using plt.subplots, we can specify how many plottable regions we want to create through nrows and ncols
    # Here we are creating a subplot with 2 columns and 1 row (i.e. side-by-side axes)
    # When we do this, axes becomes a list of length 2 (Containing both plottable axes)

    '''
      # fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 8)) creates a figure (fig) and a list of axes (axes) using Matplotlib's plt.subplots.
      # nrows=1 specifies one row of subplots.
      # ncols=2 specifies two columns of subplots, which means the images will be displayed side-by-side.
      # figsize=(12, 8) sets the size of the figure in inches.
    '''

    '''
      # fig represents the entire visualization window.
      # axes is a list containing individual plotting areas (subplots) within the figure. The number of elements in the list corresponds to the number of subplots created.
      # Together, fig and axes provide the foundation for creating your visualization using Matplotlib.
    '''

    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))

    # TODO: Call imshow on each of the axes with the first and second images
    # Make sure you handle both RGB and grayscale images

    '''
      # Checking Image Dimension:

      # if first_image.ndim == 2: This condition checks the number of dimensions (ndim) of the image.
      # If the image has two dimensions (ndim == 2), it indicates a grayscale image.
    '''

    # axes[0].imshow(first_image): This displays the first_image on the first subplot (axes[0]) using Matplotlib's imshow function.
    # cmap='gray': This argument specifies the colormap to be used for displaying the grayscale image. A grayscale colormap is appropriate for images that only have intensity values.
    # vmin=0 and vmax=255: These arguments set the minimum and maximum intensity values for the colormap. In this case, they ensure the grayscale values range from black (0) to white (255).

    if first_image.ndim == 2:
       axes[0].imshow(first_image, cmap='gray', vmin=0, vmax=255)
    else:
        axes[0].imshow(first_image)

    if second_image.ndim == 2:
        axes[1].imshow(second_image, cmap='gray', vmin=0, vmax=255)
    else:
        axes[1].imshow(second_image)

    plt.show()

# these colours are used to draw boxes.

COLOURS = [
    tuple(int(colour_hex.strip('#')[i:i+2], 16) for i in (0, 2, 4))
    for colour_hex in plt.rcParams['axes.prop_cycle'].by_key()['color']
]

from google.colab import drive
drive.mount('/content/drive')

# I have two images, a left an a right image with my iphone camera. I am holding the camera with my hand
# so it is not an exact grid

d_calib = "50cm"

path = '/content/drive/My Drive/StereoCameraImages/2Images/'

left_eye = path + 'left_eye_' + d_calib + '.jpg' #'left_eye_demo2.jpg'
right_eye = path + 'right_eye_' + d_calib + '.jpg' #'right_eye_demo2.jpg'


left_img = load_img(left_eye)
# left_img = cv2.resize(left_img, dsize=(sz1,sz2), interpolation=cv2.INTER_LINEAR)

right_img = load_img(right_eye)
# right_img = cv2.resize(right_img, dsize=(sz1,sz2), interpolation=cv2.INTER_LINEAR)

'''
  # img.shape returns a tuple containing the image dimensions.
  # [1] accesses the second element in the tuple, which corresponds to the width.
  # [0] accesses the first element, which corresponds to the height
'''

sz1 ,sz2 = right_img.shape[1], right_img.shape[0]

display_image_pair(left_img, right_img)

imgs = [left_img, right_img]

left_right = [preprocess_image(d).squeeze(dim=0) for d in imgs]

print(right_img.shape)

"""## do object detection with MaskRCNN network."""

# we use the default weights and set up the model

weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT
model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=weights)
_ = model.eval()

# this functions returns the detections
# det is the boxes, top left and bottom right cooridinates
# lbls are the class labels
# scores are the confidence. We use 0.5 as default
# masks are the segmentation masks.

def get_detections(maskrcnn, imgs, score_threshold=0.5): #person, dog, elephan, zebra, giraffe, toilet
    ''' Runs maskrcnn over all frames in vid, storing the detections '''
    # Record how long the video is (in frames)
    det = []
    lbls = []
    scores = []
    masks = []

    for img in imgs:
        with torch.no_grad():
            result = maskrcnn(preprocess_image(img))[0]
            '''
            => result = maskrcnn(preprocess_image(img))[0]: This line passes the preprocessed image (preprocess_image(img))
                    through the Mask R-CNN model (maskrcnn). The [0] indexing extracts the first element of the returned result,
                    which likely contains the model's predictions.
            '''

        mask = result["scores"] > score_threshold
        # print(result)
        # print(mask)
        '''
        => mask = result["scores"] > score_threshold: This line creates a mask that filters the detections based on their
                confidence scores (result["scores"]). Only detections with scores higher than the specified
                score_threshold (default 0.5) are kept.
        '''

        # boxes = result["boxes"][mask].detach().cpu().numpy()
        # det.append(boxes)

        '''
        => The line boxes = result["boxes"][mask].detach().cpu().numpy() in the get_detections function extracts the
                bounding boxes for the detections that pass the confidence score threshold. Here's a detailed breakdown:
        '''
        # list["columName"]["condiction"]
        det.append(result["boxes"][mask].detach().cpu().numpy())
        lbls.append(result["labels"][mask].detach().cpu().numpy())
        scores.append(result["scores"][mask].detach().cpu().numpy())
        masks.append(result["masks"][mask]) #I want this as a tensor
        # masks.append(result["masks"][mask].detach().cpu().numpy())
        # det is bounding boxes, lbls is class labels, scores are confidences and masks are segmentation masks

    print(lbls)
    return det, lbls, scores, masks

#det[0] are the bounding boxes in the left image
#det[1] are the bounding boxes in the right image

det, lbls, scores, masks = get_detections(model,imgs)

print(np.array(weights.meta["categories"])[lbls[0]])
print(np.array(weights.meta["categories"])[lbls[1]])

"""### Do some visualisation"""

#draws the bounding boxes
def draw_detections(img, det, colours=COLOURS, obj_order = None):
  # i starts from 0, len(det), (tlx, tly, brx, bry) are postion
    for i, (tlx, tly, brx, bry) in enumerate(det):
        if obj_order is not None and len(obj_order) < i:
            i = obj_order[i]
        i %= len(colours)
        c = colours[i]
        # cv2.rectangle() use to draw the rectangle for img
        cv2.rectangle(img, (tlx, tly), (brx, bry), color=colours[i], thickness=2)


#annotate the class labels
def annotate_class(img, det, lbls, conf=None, colours=COLOURS, class_map=weights.meta["categories"]):
    for i, ( tlx, tly, brx, bry) in enumerate(det):
        txt = class_map[lbls[i]]
        if conf is not None:
            txt += f' {conf[i]:1.3f}'
        # A box with a border thickness draws half of that thickness to the left of the
        # boundaries, while filling fills only within the boundaries, so we expand the filled
        # region to match the border
        offset = 1

        # again drawing the rectangle with filled color inside
        cv2.rectangle(img,
                      (tlx-offset, tly-offset+12),
                      (tlx-offset+len(txt)*12, tly),
                      color=colours[i%len(colours)],
                      thickness=cv2.FILLED)

        # writing the text that is lable in side filled rectangle in white color text
        ff = cv2.FONT_HERSHEY_PLAIN
        cv2.putText(img, txt, (tlx, tly-1+12), fontFace=ff, fontScale=1.0, color=(255,)*3)


def draw_instance_segmentation_mask(img, masks):
    ''' Draws segmentation masks over an img '''
    seg_colours = np.zeros_like(img, dtype=np.uint8)
    for i, mask in enumerate(masks):
        col = (mask[0, :, :, None] * COLOURS[i])
        seg_colours = np.maximum(seg_colours, col.astype(np.uint8))
    cv2.addWeighted(img, 0.75, seg_colours, 0.75, 1.0, dst=img)

fig, axes = plt.subplots(1, 2, figsize=(12, 8))
# imgs1 = imgs.copy()

for i, imgi in enumerate(imgs):
    img = imgi.copy()
    deti = det[i].astype(np.int32)
    draw_detections(img,deti)
    masksi = masks[i].detach().cpu().numpy()
    # print(masksi)
    annotate_class(img,deti,lbls[i])
    # draw_instance_segmentation_mask(img, masksi)
    axes[i].imshow(img)
    axes[i].axis('off')
    axes[i].set_title(f'Frame #{i}')

#draw with masks

fig, axes = plt.subplots(1, 2, figsize=(12, 8))
# imgs2 = imgs.copy()

for i, imgi in enumerate(imgs):
    img = imgi.copy()

    deti = det[i].astype(np.int32)
    draw_detections(img,deti)
    masks[i][masks[i]<0.7]=0
    masksi = masks[i].detach().cpu().numpy()
    annotate_class(img,deti,lbls[i])
    draw_instance_segmentation_mask(img, masksi)
    axes[i].imshow(img)
    axes[i].axis('off')
    axes[i].set_title(f'Frame #{i}')

#get centr, top left and bottom right of boxes

def tlbr_to_center1(boxes):
    points = []
    for tlx, tly, brx, bry in boxes:
        cx = (tlx+brx)/2
        cy = (tly+bry)/2
        points.append([cx, cy])
    return points

def tlbr_to_corner(boxes):
    points = []
    for tlx, tly, brx, bry in boxes:
        cx = (tlx+tlx)/2
        cy = (tly+tly)/2
        points.append((cx, cy))
    return points

def tlbr_to_corner_br(boxes):
    points = []
    for tlx, tly, brx, bry in boxes:
        cx = (brx+brx)/2
        cy = (bry+bry)/2
        points.append((cx, cy))
    return points

def tlbr_to_area(boxes):
    areas = []
    for tlx, tly, brx, bry in boxes:
        cx = (brx-tlx)
        cy = (bry-tly)
        areas.append(abs(cx*cy))
    return areas

'''
    # [[159.40157   27.520174 241.03233  226.7395  ]
    # [  0.         0.       112.57273  157.12044 ]
    # [240.61845  168.571    270.16025  219.13237 ]]


    # it in the form of[[Tx1,Ty1,Bx1,By1],[Tx2,Ty2,Bx2,By2],[Tx3,Ty3,Bx3,By3]]
    # where Tx : top x-axis position
    # where Ty : top y-axis position
    # where Bx : bottom x-axis position
    # where By : bottom y-axis position
    # where the 1,2,3...n are the n number item detected

'''
print(det[0],("\n")*3)
print(tlbr_to_center1(det[0]),"\n\n\n")
print(np.array(tlbr_to_center1(det[0])),"\n\n\n")
print(np.array(tlbr_to_center1(det[0]))[:,0])
print(np.array(tlbr_to_center1(det[1]))[:,0])
# this gives the center point position of x-axis of each item detected
# [200.21694946  56.28636551 255.38934326]

#get all distances from every object box to every other object box
#left image is boxes[0]
#right image is boxes[1]

#do broad casting.
#in python, col vector - row vector gives matrix:
# [a] - [c,d] = [a-c, a-d]
# [b]           [b-c, b-d]

def get_horiz_dist_centre(boxes):

    '''
        => boxes[0] gives the position of Tlx Tly Brx Bry of all the object in left images
        => boxes[1] gives the position of Tlx Tly Brx Bry of all the object in right images
    '''
    print("boxes[0]\n",boxes[0],"\n")
    print(boxes[1],"\n\n\n\n")



    '''
        => tlbr_to_center1(boxes[0])
            gives the x-axis and y-axis middle position of all the object in left images ->
                [[200.21694946289062, 127.12983703613281], [56.2863655090332, 78.56021881103516], [255.38934326171875, 193.8516845703125]]

        => tlbr_to_center1(boxes[1])
            gives the x-axis and y-axis middle position of all the object in right images ->
                [[161.3457489013672, 122.59819793701172], [216.7086181640625, 187.50534057617188], [38.904781341552734, 77.45162963867188]]
    '''
    print("tlbr_to_center1(boxes[0])\n",tlbr_to_center1(boxes[0]),"\n")
    print(tlbr_to_center1(boxes[1]),"\n\n\n\n")



    '''
        => np.array(tlbr_to_center1(boxes[0]))
                gives the x-axis middle position of all the object in left images == [200.21694946  56.28636551 255.38934326]

        => np.array(tlbr_to_center1(boxes[1]))
                gives the x-axis middle position of all the object in right images == [161.3457489  216.70861816  38.90478134]
    '''
    print("np.array(tlbr_to_center1(boxes[0]))\n",np.array(tlbr_to_center1(boxes[0])),"\n")
    print(np.array(tlbr_to_center1(boxes[1])),"\n\n\n\n")



    '''
        => np.array(tlbr_to_center1(boxes[0]))[:,0]
                gives the x-axis middle position of all the object in left images == [200.21694946  56.28636551 255.38934326]

        => np.array(tlbr_to_center1(boxes[1]))[:,0]
                gives the x-axis middle position of all the object in right images == [161.3457489  216.70861816  38.90478134]
    '''
    pnts1 = np.array(tlbr_to_center1(boxes[0]))[:,0]
    pnts2 = np.array(tlbr_to_center1(boxes[1]))[:,0]
    print("np.array(tlbr_to_center1(boxes[0]))[:,0]\n",pnts1,"\n")
    print(pnts2,"\n\n\n\n")



    '''
        pnts1 ->[200.21694946  56.28636551 255.38934326]
        pnts2 ->[161.3457489  216.70861816  38.90478134]


                [a] - [d,e,f] = [a-d, a-e ,a-f]
                [b]             [b-d, b-e ,b-f]
                [c]             [c-d, c-e ,c-f]


        pnts1[:,None] this convert the 1Dimensional array to 2Dimensional ->
                [[200.21694946]
                [ 56.28636551]
                [255.38934326]] This is like    [a]
                                                [b]
                                                [c]

        pnts2[None] this convert the 1Dimensional array to 2Dimensional ->
                [[161.3457489  216.70861816  38.90478134]] This is like [d,e,f]
    '''
    print("pnts1[:,None]: \n",pnts1[:,None],"\n")
    print("pnts2[None]  : \n",pnts2[None],"\n\n\n\n")

    print("pnts1[:,None] - pnts2[None]\n = ",pnts1[:,None] - pnts2[None],'\n\n\n\n')
    return pnts1[:,None] - pnts2[None]

def get_horiz_dist_corner_tl(boxes):
    pnts1 = np.array(tlbr_to_corner(boxes[0]))[:,0]
    pnts2 = np.array(tlbr_to_corner(boxes[1]))[:,0]
    return pnts1[:,None] - pnts2[None]

def get_horiz_dist_corner_br(boxes):
    pnts1 = np.array(tlbr_to_corner_br(boxes[0]))[:,0]
    pnts2 = np.array(tlbr_to_corner_br(boxes[1]))[:,0]
    return pnts1[:,None] - pnts2[None]

def get_vertic_dist_centre(boxes):
    pnts1 = np.array(tlbr_to_center1(boxes[0]))[:,1]
    pnts2 = np.array(tlbr_to_center1(boxes[1]))[:,1]
    return pnts1[:,None] - pnts2[None]

def get_area_diffs(boxes):
    pnts1 = np.array(tlbr_to_area(boxes[0]))
    pnts2 = np.array(tlbr_to_area(boxes[1]))
    return abs(pnts1[:,None] - pnts2[None])

# print(det)
_=get_horiz_dist_centre(det)

## get distance bentween corner and centre

centre = sz1/2

def get_dist_to_centre_tl(box, cntr = centre):
    pnts = np.array(tlbr_to_corner(box))[:,0]
    return abs(pnts - cntr)


def get_dist_to_centre_br(box, cntr = centre):
    '''
        => np.array(tlbr_to_corner_br(box))[:,0] means gets the brX position of all the object in image  ->
            [241.03233337 112.57273102 270.1602478 ]
        => np.array(tlbr_to_corner_br(box))[:,0] means gets the brX position of all the object in image ->
            [201.73226929 229.75834656  77.62784576]
    '''
    pnts = np.array(tlbr_to_corner_br(box))[:,0]



    print("pnts =",pnts," cntr =",cntr,"\n")



    '''
        abs(pnts - cntr)  Finds the distance between blx position to center of x-axis position(it is 200pixle) -> [41.03233337 87.42726898 70.1602478 ]
                            [  1.73226929  29.75834656 122.37215424]
    '''
    return abs(pnts - cntr)

tmp1 = get_dist_to_centre_br(det[0])
print(tmp1)

tmp2 = get_dist_to_centre_br(det[1])
print(tmp2)

# create the tracking cost function.
# consists of theree parts.
#  1. The vertical move up and down of object centre of mass. Scale this up because we do not expect this to be very much.
#  2. The move left or right by the object. We only expect it to move right (from the left eye image). So penalise if it moves left.
#  3. The difference in area of pixels. Area of image is width x height, so divide by height, there for this will have max value of width

def get_cost(boxes, lbls = None, sz1 = 400):
    alpha = sz1; beta  = 10; gamma = 5
    print("boxes = ",boxes,"\n\n\n")



    #vertical_dist, scale by gamma since can't move up or down
    '''
        => It scales the absolute vertical distance by a factor gamma (default 5),
                emphasizing the importance of minimizing vertical movement.
        => This is because objects are generally less likely to move significantly up or down between frames.
    '''
    print("get_vertic_dist_centre(boxes) = ",get_vertic_dist_centre(boxes),"\n")
    vert_dist = gamma*abs(get_vertic_dist_centre(boxes))
    print("gamma*abs(get_vertic_dist_centre(boxes)) = ",vert_dist,"\n\n\n")



    #horizonatl distance.
    horiz_dist = get_horiz_dist_centre(boxes)
    print("get_horiz_dist_centre = ",horiz_dist,"\n")



    #increase cost if object has moved from right to left.
    '''
        => This part penalizes objects that move leftward (negative horizontal distance).
        => It checks where horiz_dist elements are less than 0 (indicating a leftward move).
        => For those elements, it replaces their values with beta (default 10) times the absolute-
                -value of the original distance. This increases the cost associated with leftward movement.
    '''
    horiz_dist[horiz_dist<0] = beta*abs(horiz_dist[horiz_dist<0])
    print("beta*abs(horiz_dist[horiz_dist<0]) = ",horiz_dist,"\n\n\n")



    #area of box
    '''
        => It assumes a function get_area_diffs(boxes) exists to calculate the difference in area between corresponding bounding boxes.
        => It divides the area difference by alpha (default 400), likely to normalize the values within a reasonable range.
    '''
    area_diffs = get_area_diffs(boxes)/alpha
    print("get_area_diffs(boxes) = ",get_area_diffs(boxes),'\n')
    print("area_diffs = get_area_diffs(boxes)/alpha = ",area_diffs,'\n\n\n')


    '''
        => This line creates a NumPy array named cost by stacking vert_dist, horiz_dist, and area_diffs vertically.
                Each row in cost represents the corresponding cost components
                (vertical distance, horizontal distance, area difference) for a particular object pair.
    '''
    cost = np.array([vert_dist,horiz_dist,area_diffs])
    print("cost = np.array([vert_dist,horiz_dist,area_diffs]) =",cost,"\n\n")



    '''
        example
        cost = np.array([
                        [1, -2, 10],
                        [3,  1,  5],
                        [2,  4,  7]
                    ])

        cost.sum(axis=0) --> cost = np.array([6, 3, 22])
        6=>1+2+3,   3=>-2+1+4,     22=>10+5+7
        cost.sum(axis=1) --> cost = np.array([
                                            [9],
                                            [9],
                                            [13]
                                        ])
        9=>1-2+10   9=>3+1+5    13=>2+4+7

        This commented section provides an example of a cost array and demonstrates how the .sum method with different axis values works:
            => cost.sum(axis=0) (commented): Sums the elements along rows (axis 0),
                    resulting in a single row array with the total cost for each cost component
                    (vertical distance, horizontal distance, area difference) across all object pairs.
            => cost.sum(axis=1) (commented): Sums the elements along columns (axis 1).
            => This would calculate the total cost associated with each bounding box in frame 0,
                    considering all its potential assignments to different boxes in frame 1.

        => This line calculates the final cost by summing the elements in the cost array along axis 0 (rows).
        => This provides a single value representing the total cost for all object pairs,
        => considering all three cost components (vertical distance, horizontal distance, area difference).
        => The result is stored back into cost and printed for reference.
    '''
    cost=cost.sum(axis=0)
    print("cost.sum(axis=0) =",cost,"\n\n")


    #add penalty term for different object classes
    '''
        => Optional Penalty for Different Object Classes (if lbls provided):
            => This part adds a penalty term if lbls is not None. It assumes lbls is a list of lists containing object class-
                    -labels for each bounding box in each frame.
            => It iterates through the cost array and checks if the corresponding labels in lbls
                    (for the object pair represented by that element) are different.
            => If the labels differ (indicating a potential category mismatch between assigned objects),
                    it adds a penalty of 150 to the corresponding element in the cost array. This discourages assigning objects of different categories.
    '''
    '''
        => By accessing cost.shape[0], you're specifically extracting the first element from the shape tuple of the cost array.
            cost = np.array([
            [1, -2, 10],  # Cost components for object pair 1
            [3,  1,  5],  # Cost components for object pair 2
            [2,  4,  7]   # Cost components for object pair 3
            ])
        => In this case, cost.shape would be (3, 3), indicating 3 rows and 3 columns.
        => Therefore, cost.shape[0] would be equal to 3, representing the number of object pairs (rows) in the cost array.
    '''
    if lbls is not None:
        for i in range(cost.shape[0]):
            for j in range(cost.shape[1]):
                if (lbls[0][i]!=lbls[1][j]):
                    cost[i,j]+=150
    print("COST ===",cost,"\n\n\n")
    return cost

#get cost with centre of mass

# not using
def get_cost_with_com(masks, lbls = None,prob_thresh = 0.7):
    alpha = 240; beta  = 10; gamma = 5

    #left masks
    mask_bool1 = masks[0] > prob_thresh
    mask_bool1 = mask_bool1.squeeze(1)
    #right masks
    mask_bool2 = masks[1] > prob_thresh
    mask_bool2 = mask_bool2.squeeze(1)

    #com1 is center of mass of height
    #com2 is center of mass of width

    #lget eft params
    mask_size = (mask_bool1).sum(dim=[1,2])
    mask_com_matrix_1 = torch.tensor(range(mask_bool1.shape[1]))
    com1 = ((mask_com_matrix_1.unsqueeze(1))*mask_bool1).sum(dim=[1,2])/mask_size
    mask_com_matrix_2 = torch.tensor(range(mask_bool1.shape[2]))
    com2 = ((mask_com_matrix_2.unsqueeze(0))*mask_bool1).sum(dim=[1,2])/mask_size

    left_params = torch.stack((com1, com2, mask_size)).transpose(1,0)

    #get right params
    mask_size2 = (mask_bool2).sum(dim=[1,2])
    mask_com_matrix_12 = torch.tensor(range(mask_bool2.shape[1]))
    com12 = ((mask_com_matrix_12.unsqueeze(1))*mask_bool2).sum(dim=[1,2])/mask_size2
    mask_com_matrix_22 = torch.tensor(range(mask_bool2.shape[2]))
    com22 = ((mask_com_matrix_22.unsqueeze(0))*mask_bool2).sum(dim=[1,2])/mask_size2

    right_params = torch.stack((com12, com22, mask_size2)).transpose(1,0)

    #calculate cost function
    cost = (left_params[:,None] - right_params[None])
    #scale counts
    cost[:,:,2]=abs(cost[:,:,2])/alpha

    #can't move right, can only move left
    cost[cost[:,:,1]<0] = beta*abs(cost[cost[:,:,1]<0])

    #move up and down, take abs vals
    cost[:,:,0] = gamma*abs(cost[:,:,0])
    # print(cost.shape)
    cost = cost.sum(dim=2)
    if lbls is not None:
        for i in range(cost.shape[0]):
            for j in range(cost.shape[1]):
                if (lbls[0][i]!=lbls[1][j]):
                    cost[i,j]+=100
                    # print(lbls[0][i], lbls[1][j])
    print("cost =========",cost)
    return cost


def get_horiz_dist(masks, prob_thresh = 0.7):
    # gets the horizontal distance between the centre of mass for each object
    #left masks
    mask_bool1 = masks[0] > prob_thresh
    mask_bool1 = mask_bool1.squeeze(1)
    #right masks
    mask_bool2 = masks[1] > prob_thresh
    mask_bool2 = mask_bool2.squeeze(1)

    #left params
    #com1 is center of mass of height
    #com2 is center of mass of width
    mask_size = (mask_bool1).sum(dim=[1,2])
    mask_com_matrix_1 = torch.tensor(range(mask_bool1.shape[1]))
    com1 = ((mask_com_matrix_1.unsqueeze(1))*mask_bool1).sum(dim=[1,2])/mask_size
    mask_com_matrix_2 = torch.tensor(range(mask_bool1.shape[2]))
    com2 = ((mask_com_matrix_2.unsqueeze(0))*mask_bool1).sum(dim=[1,2])/mask_size

    left_params = torch.stack((com1, com2, mask_size)).transpose(1,0)

    #get right params
    mask_size2 = (mask_bool2).sum(dim=[1,2])
    mask_com_matrix_12 = torch.tensor(range(mask_bool2.shape[1]))
    com12 = ((mask_com_matrix_12.unsqueeze(1))*mask_bool2).sum(dim=[1,2])/mask_size2
    mask_com_matrix_22 = torch.tensor(range(mask_bool2.shape[2]))
    com22 = ((mask_com_matrix_22.unsqueeze(0))*mask_bool2).sum(dim=[1,2])/mask_size2

    right_params = torch.stack((com12, com22, mask_size2)).transpose(1,0)

    #calculate cost function
    cost = (left_params[:,None] - right_params[None])
    return cost[:,:,1]

# not used
def get_tracks(cost):
    print("get_tracks(cost) =",cost)
    print("get_tracks(cost) = scipy.optimize.linear_sum_assignment(cost) =",scipy.optimize.linear_sum_assignment(cost),"\n\n\n")
    return scipy.optimize.linear_sum_assignment(cost)


def get_tracks_ij(cost):
    tracks = scipy.optimize.linear_sum_assignment(cost)
    return [[i,j] for i, j in zip(*tracks)]

'''
    => Overall, the get_cost function calculates a cost matrix that reflects the suitability of assigning bounding-
            -boxes from one frame to another in an object tracking application.
    => It considers factors like vertical and horizontal movement, area difference, and optionally, object category compatibility.
    => This cost matrix is then used by subsequent algorithms to find the optimal assignments for objects between frames,
            achieving consistent and accurate object tracking.
    => A high value in the cost function calculated by the get_cost function generally indicates a poor-
            -match between a pair of bounding boxes from two consecutive frames.
    => In the context of the get_cost function, a low cost value generally indicates a good-
            -match between a pair of bounding boxes from two consecutive frames.
    => A low cost value signifies that the compared bounding boxes have similar properties,
        making them strong candidates for representing the same object across frames.
    => This suggests:
        => Minimal vertical and horizontal movement between frames.
        => Similar areas.
        => Compatibility in object categories (if lbls is used).
'''
cost = get_cost(det, lbls = lbls)

'''
    [
        [a-d, a-e, a-f]
        [b-d, b-e, b-f]
        [c-d, c-e, c-f]
    ]
'''

'''
    ==>> tracks = scipy.optimize.linear_sum_assignment(cost)

    => This function performs the Hungarian algorithm, which is an efficient method for solving the assignment problem.
    => In object tracking, the assignment problem involves finding the optimal assignment of bounding boxes from frame 0 to those in frame 1,
            minimizing the total cost across all assignments.

    tracks is a tuple containing two elements:
        => The first element represents the row indices (corresponding to bounding boxes in frame 0)
            assigned to the columns (bounding boxes in frame 1).
        => The second element represents the corresponding cost values for each assignment.
            tracks = (array([0, 1, 2]), array([0, 2, 1]))
                (array([0, 1, 2]) ->  first element
                (array([0, 2, 1]) -> second element


    ==>> h_d = [[np.array(weights.meta["categories"])[lbls[0]][i],np.array(weights.meta["categories"])[lbls[1]][j]] for i, j in zip(*tracks)]

    Extracting Object Categories (if lbls is provided):
        => It iterates through the tracks tuple, unpacking the row and column indices using zip(*tracks).

        For each assigned pair of indices (i, j) representing a bounding box from frame 0 and its assigned box in frame 1:
            => It extracts the object category label from np.array(weights.meta["categories"])[lbls[0]][i].
            => This accesses the category label for the i-th bounding box in frame 0 from the weights.meta["categories"] dictionary
                    (assumed to store category information).
            => Similarly, it retrieves the category label for the assigned bounding box in frame 1.
            => It creates a sub-list containing these two category labels and appends it to the h_d list.

        => This process essentially pairs up the object category labels for each assigned track (bounding box correspondence).


    ==>> print(np.array(weights.meta["categories"])[lbls[0]])
        => The first line prints all the object category labels for bounding boxes in frame 0
                (assuming lbls[0] holds those labels). This provides context for the category pairs in h_d.
'''
tracks = scipy.optimize.linear_sum_assignment(cost)
print("tracks =",tracks)

h_d = [[np.array(weights.meta["categories"])[lbls[0]][i]+" Left",np.array(weights.meta["categories"])[lbls[1]][j]+" Right"] for i, j in zip(*tracks)]
print(np.array(weights.meta["categories"])[lbls[0]],np.array(weights.meta["categories"])[lbls[1]])
print(h_d)

"""## The distance between the left and right centres is given by dists for each object.

##### We use the geometric formulas to calibrate the tantheta and focal lenght of the camera for 30cm and 50cm bottle and then transform the pixels differences to distance in cm's for each object in the image
"""

#we take the corner that is closest to the centre. This is because the other corner might be going off the image.

dists_tl =  get_horiz_dist_corner_tl(det)
dists_br =  get_horiz_dist_corner_br(det)

print("dists_tl =",dists_tl,"\n")
print("dists_br =",dists_br,"\n\n\n")

final_dists = []
dctl = get_dist_to_centre_tl(det[0])
dcbr = get_dist_to_centre_br(det[0])

print("dctl =",dctl)
print("dcbr =",dcbr,"\n\n\n")

for i, j in zip(*tracks):
    if dctl[i] < dcbr[i]:
        final_dists.append((dists_tl[i][j],np.array(weights.meta["categories"])[lbls[0]][i]))
    else:
        final_dists.append((dists_br[i][j],np.array(weights.meta["categories"])[lbls[0]][i]))

final_dists

# get focal length
# for 50cm away bottle image, we had 38.44 pixels between bottle boxes
# and for 30cm away bottle image we had 68.75 pixels between left and right bottles
fl = 30-38.44*50/68.75
print(fl)
'''
    => The code you provided calculates the focal length (fl) of a camera based on the distance between an object
            in two images and the corresponding pixel separation between the object in those images.

    Assumptions:
        => You have two images of the same object taken from different distances.
        => The object is relatively close to the camera compared to the background.
        => The object's size in pixels is measured consistently between the two images (e.g., always measuring the width between bounding boxes).

    Calculation Steps:
        Known Distances:
            => You have two values:
            => known_distance_1: Distance to the object in the first image (50 cm in this case).
            => known_distance_2: Distance to the object in the second image (30 cm).

        Pixel Separations:
            => You have two values representing the pixel separation between the object in the images:
            => pixel_separation_1: Pixel separation in the first image (38.44 pixels).
            => pixel_separation_2: Pixel separation in the second image (68.75 pixels).

        Focal Length Formula (Simplified):
            => The code uses a simplified version of the thin lens formula that relates the object distance (d_object),
            => image distance (d_image), and focal length (f) as follows:
            => 1/f = 1/d_object + 1/d_image
'''

known_distance_1 = 50  # Distance to the bottle in the first image (cm)
known_distance_2 = 30  # Distance to the bottle in the second image (cm)
pixel_separation_1 = 38.44  # Pixel separation between bottle boxes in first image
pixel_separation_2 = 68.75  # Pixel separation between bottle boxes in second image

# Assuming a linear relationship between distance and pixel separation
focal_length = known_distance_2 - (pixel_separation_1 * known_distance_1) / pixel_separation_2

print("Calculated focal length:", focal_length, "cm")

#calibrate theta. cameras are 7.05 cms apart
baseLine=7.05
tantheta = (1/(known_distance_1-focal_length))*(baseLine/2)*(sz1/pixel_separation_1)
print("Tan(x) =",tantheta)
fd = [i for (i,j) in final_dists]
val=focal_length*baseLine/(np.array(fd)*tantheta)
print(val)

#final distances as list
fd = [i for (i,j) in final_dists]
print(fd)

#find distance away
dists_away = (baseLine/2)*sz1*(1/tantheta)/np.array(fd)+focal_length
print(dists_away)

cat_dist = []
for i in range(len(dists_away)):
    cat_dist.append(f'{np.array(weights.meta["categories"])[lbls[0]][i]} {dists_away[i]:.1f}cm')
    print(f'{np.array(weights.meta["categories"])[lbls[0]][i]} is {dists_away[i]:.1f}cm away')
print("\n\ncat_dist =",cat_dist)

#annotate the class labels
def annotate_class2(img, det, lbls,class_map, conf=None,  colours=COLOURS):
    for i, ( tlx, tly, brx, bry) in enumerate(det):
        txt = class_map[i]
        if conf is not None:
            txt += f' {conf[i]:1.3f}'
        # A box with a border thickness draws half of that thickness to the left of the
        # boundaries, while filling fills only within the boundaries, so we expand the filled
        # region to match the border
        offset = 1

        cv2.rectangle(img,
                      (tlx-offset, tly-offset+12),
                      (tlx-offset+len(txt)*12, tly),
                      color=colours[i%len(colours)],
                      thickness=cv2.FILLED)

        ff = cv2.FONT_HERSHEY_PLAIN
        cv2.putText(img, txt, (tlx, tly-1+12), fontFace=ff, fontScale=1.0, color=(255,)*3)

list(np.array(cat_dist)[(tracks[0])])

fig, axes = plt.subplots(1, 2, figsize=(12, 8))

t1 = [list(tracks[1]), list(tracks[0])]

for i, imgi in enumerate(imgs):
    img = imgi.copy()
    deti = det[i].astype(np.int32)
    draw_detections(img,deti[list(tracks[i])], obj_order=list(t1[i]))
    annotate_class2(img,deti[list(tracks[i])],lbls[i][list(tracks[i])],cat_dist)
    axes[i].imshow(img)
    axes[i].axis('off')
    axes[i].set_title(f'Frame #{i}')

